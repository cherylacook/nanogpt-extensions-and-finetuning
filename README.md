# NanoGPT Extensions and Fine-tuning

## Objective:
To explore and extend the NanoGPT codebase by:
- analysing token-level probability distributions during generation,
- computing full-sequence probabilities using log-likelihoods,
- adding a fixed response mode and evaluating fixed response probabilities,
- and fine-tuning GPT-2 on a specialised dataset to observe domain adaptation.

This repository contains only my modifications. The original NanoGPT codebase: https://github.com/karpathy/nanoGPT 

## Structure:

## How to Run:



